{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        CONTEXTE\n",
    "\n",
    "Le fichier DVF (Demandes de Valeurs Foncières) produit par le 'Ministère de l'Économie, des Finances et de la Souveraineté industrielle et numérique' est maintenant accessible en open data, en conformité avec un décret de décembre 2018 relatif à la publication électronique d'informations sur les valeurs foncières déclarées lors de mutations immobilières. La publication de ces informations vise à rendre les marchés fonciers et immobiliers plus transparents. Le fichier comportant des informations à caractère personnel, son utilisation nécessite certaines précautions : \n",
    "\n",
    "1. ne doit pas permettre de ré-identifier les personnes concernées, de manière indirecte\n",
    "2. ne doit pas permettre l'indexation des informations sur les moteurs de recherche \n",
    "\n",
    "Ce jeu de données \"Demandes de valeurs foncières\", publié par la DGFiP, recense les transactions immobilières des 5 dernières années en métropole et DOM-TOM (sauf Alsace, Moselle et Mayotte). Les informations sont tirées des actes notariés et des données cadastrales.\n",
    "Une documentation détaillée comportant 'Foire aux questions', Conditions générales d'utilisation', 'Information des personnes concernées par le traitement informatique' et 'Notice descriptive des fichiers de valeurs foncières' est accessible via le site https://www.data.gouv.fr/fr/datasets/demandes-de-valeurs-foncieres/#resources. Elle permet notamment de comprendre, à travers leur description détaillée, les champs renseignés et les données présentes dans la base de données mais aussi toute information relative à l'utilisation de la base.\n",
    "\n",
    "Le premier jeu de données que nous avons utilisé dans le cadre de notre projet est dérivé du fichier DVF. Les fichiers sont produits par Etalab (département de la Direction Interministérielle du NUMérique qui coordonne la conception et la mise en œuvre de la stratégie d'open data de l’État) dans un format normalisé et enrichi. Nous avons récupéré ces données dans des fichiers aux formats csv.\n",
    "Des informations détaillées concernant les enrichissements faits par rapport à la base de données initiales ainsi que d'autres informations complémentaires peuvent être trouvées à cette adresse https://www.data.gouv.fr/fr/datasets/demandes-de-valeurs-foncieres-geolocalisees/.\n",
    "\n",
    "Le second jeu de données que nous avons utilisé a été produit par Rennes Métropole. Les données disponibles dans plusieurs formats à cette adresse https://geo.data.gouv.fr/fr/datasets/63a0ef5654cb7cf3e28684e6b7dc7ca84eaf9799 ont été mises à jour jusqu'en 2021. Une documentation succincte présente sur le site nous indique que les données sont des données géospatiales décrivant par des polygones les bâtiments de Rennes Métropole. Les bâtiments possèdent un identifiant unique composé d'un digramme (code commune + suite de 6 caractères numériques) et sont décrits par des points géolocalisés. Les données ont été collectées à partir des permis de construire (Ville de Rennes), de bases topographiques et orthophotographiques ou de restitution photogrammétrique. L'objectif de la constitution d'une telle base était de maintenir une base de référence locale pour les besoins de la ville. Rennes Métropole, précurseure de l'open data, la base créée en 2005 avait aussi pour objectif d'être mise à jour plus fréquemment que les sources de données externes comme la DGFiP. Nous avons utilisé le fichier dans son format geoJSON.\n",
    "\n",
    "Nous donnerons plus de précisions sur les bases dans la suite du dossier."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        INTRODUCTION\n",
    "\n",
    "Dans le cadre de notre projet nous souhaitions utiliser ces données de DVF. Notre objectif était de représenter ces données de manière visuelle voire interactive, grâce aux compétences acquises pendant notre Master dans la manipulation des données. Mais comment développer un outil de visualisation de données servant à l'étude des marchés immobiliers et fonciers ?\n",
    "\n",
    "Après avoir mis en place notre projet via les canaux que nous souhaitions utiliser (planification via Trello, mise en place d'un repository GitHub) nous avons commencé par nous approprier les données avec lesquelles nous souhaitions travailler. Une première exploration sommaire de notre base nous a permis de dire qu'au vu de la quantité de données à notre disposition il était préférable de nous concentrer dans un premier temps sur la ville de Rennes. Nous avons ainsi planifié notre projet dans une optique de Minimum Viable Product (MVP), c'est-à-dire avec pour objectif de réaliser dans un premier temps une version fonctionnelle présentant seulement les éléments basiques de notre projet pour ensuite venir ajouter des options. Dans le même temps, la réalisation d'un état de l'art des solutions qui nous étaient accessibles pour visualiser les données nous a mené à nous familiariser avec la librairie python folium pour représenter sur une carte nos données. Cette première exploration sommaire ainsi que nos premiers tests avec folium nous ont poussé à chercher d'autres données géospatiales afin de les croiser avec nos données pour les représenter plus finement. Nous avons alors trouvé la base de données produite par Rennes Métropole représentant les bâtiments par des polygones de la librairie python shapely.\n",
    "\n",
    "Notre première tentative de production visait donc à lier la base de données des valeurs foncières aux bâtiments afin de représenter ces données via folium. Cependant, les cartes créées avec folium étaient particulièrement lourdes et donc très peu agréables à explorer. C'est pourquoi, nous avons finalement trouvé notre MVP lorsque nous avons opté pour l'utilisation de Tableau pour la partie visualisation de notre projet."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        PRE-TRAITEMENT DES DONNEES\n",
    "\n",
    "La première partie de notre projet consiste en le traitement de nos deux bases de données afin de les rendre utilisables lors de la visualisation. En nous intéressant à la documentations et en explorant les données, nous avons pu établir un schéma décrivant les étapes clés de notre projet (cf. Schéma du traitement de nos données) et déterminer les pré-traitements à effectuer sur les données.\n",
    "\n",
    "![Schéma du traitement de nos données](schema.jpg)\n",
    "\n",
    "Pour les données DVF, nous nous sommes concentrés sur le fichier propre au département de l'Ille-et-Vilaine disponible via à l'adresse https://files.data.gouv.fr/geo-dvf/latest/csv/2022/departements/. Le fichier comporte plus de 35000 lignes. L'exploration du fichier nous a permis de déterminer les dimensions de la base DVF qui nous intéressaient à savoir :\n",
    "- 'id_mutation' : id de l'élément\n",
    "- 'date_mutation' : dates de l'élément dans le format yyyy-mm-dd\n",
    "- 'nature_mutation' : prend les valeurs 'Vente' ou 'Vente en l'état futur d'achèvement'\n",
    "- 'valeur_fonciere' : valeur de l'élément\n",
    "- 'adresse_numero', 'adresse_suffixe', 'adresse_nom_voie', 'adresse_code_voie', 'code_postal', 'code_commune', 'nom_commune', 'code_departement' : les informations relatives à l'adresse de l'élément (qui ne sera en réalité pas utilisée et éliminée)\n",
    "- 'id_parcelle' :  identifiant de parcelle compatible avec les fichiers cadastraux proposés par Etalab\n",
    "- 'type_local' : prend les valeurs 'Maison', 'Appartement', 'Dépendance' ou 'Local industriel. commercial ou assimilé' et décrit le type de local\n",
    "- 'longitude', 'latitude' : Géocodage latitude/longitude à la parcelle en coordonnées WGS-84\n",
    "\n",
    "Dans un premier traitement simplifié qui servira d'exemple pour illustrer cette partie, nous avons sélectionné uniquement les dimensions 'valeur_fonciere', 'longitude' et 'latitude'. Dans un second traitement de nos données, nous avons ensuite enrichie la base créée pour la visualisation avec les dimmensions 'id_mutation', 'date_mutation',  'nature_mutation', 'adresse_numero', 'adresse_suffixe', 'adresse_nom_voie', 'adresse_code_voie', 'code_postal', 'code_commune', 'nom_commune', 'code_departement', 'id_parcelle', 'type_local', 'nature_mutation' et 'type_local'. Le but étant de pouvoir visualiser et analyser les données de valeur foncières sous différents paramètres.\n",
    "\n",
    "Pour les données de référentiels des bâtiments à Rennes, nous avons utilisé le fichier geoJSON accessible via l'adresse mentionnée plus haut. L'exploration sommaire du fichier nous a permis de comprendre sa structure afin de déterminer quels étaient les champs qui nous intéressaient. Le fichier comporte plus de 145000 éléments. Nous avons choisi de ne garder pour cette base que le champ 'geometry' des éléments qui sont des types d'objets de la librairie shapely.\n",
    "\n",
    "Notre objectif était alors d'associer aux valeurs foncières de l'Ille-et-Vilaine un forme géométrique lorsque c'est possible. \n",
    "\n",
    "Avant de pouvoir réaliser une fonction qui ferait l'association, il était donc nécessaire de préparer nos données afin de pouvoir réaliser nos opérations dessus. Pour la première dataframe issue du chargement du fichier 35.csv, il a fallu via un script :\n",
    "1. éliminer les colonnes qui ne comportaient aucune valeur (inser. ligne code ici)\n",
    "2. éliminer les doublons\n",
    "3. éliminer toutes les colonnes sauf 'valeur_fonciere', 'longitude' et 'latitude'\n",
    "4. éliminer les lignes ayant des valeurs manquante dans au moins une des colonnes gardées\n",
    "5. transformer la data frame en une geodata frame en se servant de 'longitude' et 'latitude' pour créer un Point shapely à mettre dans le champ 'geometry' de chaque élément\n",
    "6. définir la projection pour la geodataframe via l'attribut crs\n",
    "7. sauvegarder la geodataframe dans un fichier geoJSON pour pouvoir utiliser le résultat du pré-traitement\n",
    "\n",
    "De la même manière, il a fallu préparer les données de la seconde data frame issue du chargement des données de référentiels bâtiments de la ville de Rennes. Nous avons donc réaliser un script permettant de :\n",
    "1. sauvegarder dans un fichier les 3 premiers éléments de la geodataframe\n",
    "2. ne garder que la colonne 'geometry'\n",
    "3. supprimer les doublons\n",
    "4. supprimer les éléments n'ayant pas de champ 'geometry' rempli\n",
    "5. sauvegarder la geodataframe dans un fichier geoJSON pour pouvoir utiliser le résultat du pré-traitement\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        PRODUCTION DE NOTRE BASE\n",
    "\n",
    "Une fois que nous avons préparé nos deux bases de données, il s'agissait d'associer aux données de valeurs foncières un polygone lorsque le point de la première base est compris dans le polygone de la deuxième. Il a fallu évidemment vérifier la compatibilité en termes de projection pour s'assurer que les comparaisons faites aient du sens. Avant de pouvoir réaliser cette étape, il a été nécessaire de transformer un petit peu les données contenant les polygones des bâtiments de Rennes. En effet, le format des objets contenus dans ce fichier est 'Polygon Z', c'est-à-dire que la 3ème dimension est présente. Il a donc fallu enlever cette dimension afin de pouvoir représenter plus facilement nos données ainsi que pour simplifier l'association de la base DVF aux polygones. Nous avons pour ce faire écrit la fonction 'convert_to_2d(multipolygon)' qui prend une liste de Polygones Z pour retourner une liste de polygon en 2D.\n",
    "\n",
    "Notre script s'occupe ensuite de faire l'association et de remplacer le Point de la colonne 'geometry' de la data frame DVF si celui-ci est compris dans un polygone. \n",
    "\n",
    "La base de données créée est ensuite sauvegardée dans un fichier geoJSON qui va nous servir pour la visualisation dans Tableau.\n",
    "\n",
    "Lors de nos premiers tests sur Tableau, nous n'avons pas pu visualiser les données produites car la solution ne prenait pas en compte les géometry mélangées. Il a donc fallu adapter notre code pour qu'il génère après l'association 2 fichiers différents, un pour les données possédant un point pour les géolocaliser, un autre pour les données géolocalisées par un polygone."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        VISUALISATION AVEC TABLEAU\n",
    "\n",
    "Une fois notre base créée, nous souhaitions représenter nos données via la solution Tableau. Cela nous a semblé être un choix judicieux tant cet outil se démocratise dans les métiers utilisant la data. Seulement nous avons rencontré quelques difficultés lors de la prise en main notamment pour pouvoir faire reconnaître nos données au format geoJSON (de manière à pouvoir afficher nos formes).\n",
    "\n",
    "La solution Tableau met à disposition des développeurs des solutions leur permettant d'inclure les visualisations dans des web applications ou sur des sites web. Dans une logique de MVP, cette étape pourrait intervenir avant les pistes d'évolution citées par la suite.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        *** CODE ***\n",
    "\n",
    "        -> Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import folium\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import shapely.geometry as geometry\n",
    "from shapely.geometry import Point, Polygon"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        -> Load '35.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le nombre de lignes dans ce dataframe est :  36638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\valfo\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:3378: DtypeWarning: Columns (18,20) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "# Chargement du fichier '35.csv' contenant les données de dvf geolocalisées (WGS-84)\n",
    "df = pd.read_csv(\"35.csv\")\n",
    "print(\"Le nombre de lignes dans ce dataframe est : \", df.shape[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        -> Traitement v1 du dataframe DVF : MVP\n",
    "\n",
    "Pour développer notre projet, nous avons travaillé dans une logique de MVP (Minimum Viable project). Pour ce faire, nous avons d'abord réalisé un traitement \"simplifié\" de la base DVF qui est notre base de données principale de travail. Dans ce traitement, nous avons sélectionné uniquement les colonnes 'valeur_fonciere','longitude' et 'latitude' de manière à pouvoir créer à partir des 'longitude', 'latitude' un Point shapely qui servira pour l'association aux polygones ou directement pour la visualisation (s'il n'y a pas de polygone associé)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "37\n",
      "Le nombre de lignes dans ce dataframe est :  36638\n",
      "Le nombre de lignes dans ce dataframe est :  35022\n",
      "Le nombre de lignes dans ce dataframe est :  34848\n",
      "Le nombre de lignes dans ce dataframe est :  34647\n",
      "Le nombre de lignes dans ce dataframe est :  34647\n",
      "None\n",
      "+init=epsg:4326 +type=crs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\valfo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyproj\\crs\\crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n"
     ]
    }
   ],
   "source": [
    "# ---> V1 <---\n",
    "df_v1 = df\n",
    "\n",
    "\n",
    "# Traitement\n",
    "\n",
    "# 1. élimine les colonnes qui n'ont aucune valeur\n",
    "print(len(df_v1.columns))\n",
    "df_v1.dropna(axis=1, how='all', inplace=True)\n",
    "print(len(df_v1.columns))\n",
    "\n",
    "\n",
    "# 2. élimine les doublons\n",
    "print(\"Le nombre de lignes dans ce dataframe est : \", df_v1.shape[0])\n",
    "df_v1.drop_duplicates(keep = 'first', inplace=True)\n",
    "print(\"Le nombre de lignes dans ce dataframe est : \", df_v1.shape[0])\n",
    "\n",
    "\n",
    "# 3. Sélection des colonnes d'intérêt uniquement\n",
    "df_v1 = df_v1[['valeur_fonciere','longitude', 'latitude']]\n",
    "\n",
    "\n",
    "# 4. élimine les lignes avec des valeurs manquante\n",
    "df_v1 = df_v1[df_v1['valeur_fonciere'].notna()]\n",
    "print(\"Le nombre de lignes dans ce dataframe est : \", df_v1.shape[0])\n",
    "df_v1 = df_v1[df_v1['longitude'].notna()]\n",
    "print(\"Le nombre de lignes dans ce dataframe est : \", df_v1.shape[0])\n",
    "df_v1 = df_v1[df_v1['latitude'].notna()]\n",
    "print(\"Le nombre de lignes dans ce dataframe est : \", df_v1.shape[0])\n",
    "\n",
    "\n",
    "# 5. transformer la dataframe en une geodataframe \n",
    "# Crée une colonne 'geometry' avec un point créé à partir de la latitude et longitude\n",
    "geometry = [Point(xy) for xy in zip(df_v1['longitude'], df_v1['latitude'])]\n",
    "# Drop des colonnes 'latitude' et 'longitude'\n",
    "df_v1 = df_v1.drop(['longitude', 'latitude'], axis=1)\n",
    "# Transformer la df en geodataframe\n",
    "#print('Transformer le dataframe en geodataframe')\n",
    "gdf_v1 = gpd.GeoDataFrame(df_v1, geometry=geometry)\n",
    "\n",
    "\n",
    "# 6. définir la projection pour la geodataframe via l'attribut crs          /!\\ à vérif. /!\\\n",
    "print(gdf_v1.crs)\n",
    "gdf_v1.crs = {'init': 'epsg:4326'}\n",
    "gdf_v1.crs = \"+init=epsg:4326\"\n",
    "print(gdf_v1.crs)\n",
    "\n",
    "\n",
    "# 7. Enregistrement de la geodataframe dans un geoJSON\n",
    "gdf_v1.to_file(\"dvf_point.geojson\", driver=\"GeoJSON\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        -> Traitement v2 du dataframe DVF : Evolution 1\n",
    "\n",
    "Afin d'enrichir les données des fichiers produits pour la visualisation, nous avons dans une 2ème version du traitement de notre base DVF sélectionné un plus grand nombre de colonne ('id_mutation', 'date_mutation',  'nature_mutation', 'valeur_fonciere', 'adresse_numero', 'adresse_suffixe', 'adresse_nom_voie', 'adresse_code_voie', 'code_postal', 'code_commune', 'nom_commune', 'code_departement', 'id_parcelle', 'type_local', 'nature_mutation', 'type_local', 'longitude' et 'latitude'). La production d'une telle base permet d'augmenter la personnalisation des visualisations produites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---> V2 <---\n",
    "df_v2 = df\n",
    "\n",
    "\n",
    "# Traitement\n",
    "\n",
    "# 1. élimine les colonnes qui n'ont aucune valeur\n",
    "print(len(df_v2.columns))\n",
    "df_v2.dropna(axis=1, how='all', inplace=True)\n",
    "print(len(df_v2.columns))\n",
    "\n",
    "\n",
    "# 2. élimine les doublons\n",
    "print(\"Le nombre de lignes dans ce dataframe est : \", df_v2.shape[0])\n",
    "df_v2.drop_duplicates(keep = 'first', inplace=True)\n",
    "print(\"Le nombre de lignes dans ce dataframe est : \", df_v2.shape[0])\n",
    "\n",
    "\n",
    "# 3. Sélection des colonnes d'intérêt uniquement\n",
    "df_v2 = df_v2[['id_mutation', 'date_mutation',  'nature_mutation', 'valeur_fonciere', 'adresse_numero', 'adresse_suffixe', 'adresse_nom_voie', 'adresse_code_voie', 'code_postal', 'code_commune', 'nom_commune', 'code_departement', 'id_parcelle', 'type_local', 'nature_mutation', 'type_local', 'longitude','latitude']]\n",
    "\n",
    "\n",
    "# 4. élimine les lignes avec des valeurs manquante\n",
    "df_v2 = df_v2[df_v2['valeur_fonciere'].notna()]\n",
    "print(\"Le nombre de lignes dans ce dataframe est : \", df_v2.shape[0])\n",
    "df_v2 = df_v2[df_v2['longitude'].notna()]\n",
    "print(\"Le nombre de lignes dans ce dataframe est : \", df_v2.shape[0])\n",
    "df_v2 = df_v2[df_v2['latitude'].notna()]\n",
    "print(\"Le nombre de lignes dans ce dataframe est : \", df_v2.shape[0])\n",
    "\n",
    "\n",
    "# 5. transformer la dataframe en une geodataframe \n",
    "# Crée une colonne 'geometry' avec un point créé à partir de la latitude et longitude\n",
    "geometry = [Point(xy) for xy in zip(df_v2['longitude'], df_v2['latitude'])]\n",
    "# Drop des colonnes 'latitude' et 'longitude'\n",
    "df_v2 = df_v2.drop(['longitude', 'latitude'], axis=1)\n",
    "# Transformer la df en geodataframe\n",
    "#print('Transformer le dataframe en geodataframe')\n",
    "gdf_v2 = gpd.GeoDataFrame(df_v2, geometry=geometry)\n",
    "\n",
    "\n",
    "# 6. définir la projection pour la geodataframe via l'attribut crs          /!\\ à vérif. /!\\\n",
    "print(gdf_v2.crs)\n",
    "gdf_v2.crs = {'init': 'epsg:4326'}\n",
    "gdf_v2.crs = \"+init=epsg:4326\"\n",
    "print(gdf_v2.crs)\n",
    "\n",
    "\n",
    "# 7. Enregistrement de la geodataframe dans un geoJSON\n",
    "gdf_v2.to_file(\"dvf_v2_point.geojson\", driver=\"GeoJSON\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        -> Load 'referentiel_batiment.geojson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du fichier 'referentiel_batiment.geojson' contenant les polygones représentant les bâtiments\n",
    "gdf2 = gpd.read_file(\"referentiel_batiment.geojson\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        -> Traitement du dataframe 'referentiel_batiment.geoJSON'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le nombre de lignes dans ce dataframe est :  147095\n",
      "Le nombre de lignes dans ce dataframe est :  147093\n",
      "Le nombre de lignes dans ce dataframe est :  147093\n"
     ]
    }
   ],
   "source": [
    "# 1. sauvegarder dans un fichier les 3 premiers éléments de la geodataframe\n",
    "df_temp = gdf2.head(3)\n",
    "df_temp.to_file(\"tete_geoJSON.geojson\", driver='GeoJSON')\n",
    "\n",
    "# 2. ne garder que la colonne 'geometry'\n",
    "gdf2 = gdf2[['geometry']]\n",
    "\n",
    "# 3. supprimer les doublons\n",
    "print(\"Le nombre de lignes dans ce dataframe est : \", gdf2.shape[0])\n",
    "gdf2 = gdf2.drop_duplicates()\n",
    "print(\"Le nombre de lignes dans ce dataframe est : \", gdf2.shape[0])\n",
    "\n",
    "# 4. supprimer les éléments n'ayant pas de champ 'geometry' rempli\n",
    "gdf2 = gdf2.dropna(subset=['geometry'])\n",
    "print(\"Le nombre de lignes dans ce dataframe est : \", gdf2.shape[0])\n",
    "\n",
    "# 5. sauvegarder la geodataframe dans un fichier geoJSON pour pouvoir utiliser le résultat du pré-traitement\n",
    "gdf2.to_file(\"polygonss.geojson\", driver=\"GeoJSON\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        -> Définition de la fonction convertissant les polygones 3d en polygones 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction qui convertit les polygon 3d en polygon 2d\n",
    "def convert_to_2d(multipolygon):\n",
    "    converted_multipolygon = []\n",
    "    for polygon in multipolygon.geoms:\n",
    "        converted_polygon = []\n",
    "        for point in polygon.exterior.coords:\n",
    "            converted_polygon.append((point[0], point[1]))\n",
    "        converted_multipolygon.append(Polygon(converted_polygon))\n",
    "    return converted_multipolygon"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        -> Préparation d'une liste contenant l'ensemble des polygones transformés en 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser une liste pour stocker les objets shape\n",
    "shapes = []\n",
    "\n",
    "# Création d'une liste contenant tous les polygones du geoJSON\n",
    "if 'geometry' in gdf2.columns:\n",
    "    # Boucle sur chaque entrée dans le fichier geoJSON\n",
    "    for i, row in gdf2.iterrows():\n",
    "        #print(type(row['geometry']))\n",
    "        #print(row['geometry'])\n",
    "        converted_multipolygon = convert_to_2d(row['geometry'])\n",
    "        #print(converted_multipolygon)\n",
    "        for e in converted_multipolygon:\n",
    "            shapes.append(e)\n",
    "else:\n",
    "    print('pb ici')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        -> Association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# association des valeurs foncières aux shapes\n",
    "for a, row in gdf_v1.iterrows():\n",
    "    for ele in shapes:\n",
    "        if row['geometry'].within(ele):\n",
    "            gdf_v1.loc[a, 'geometry'] = ele\n",
    "            print(a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        -> Sauvegarde des fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde des fichiers utilisables pour tableau\n",
    "gdf_v1.to_file(\"datas_finales.geojson\", driver=\"GeoJSON\")\n",
    "# gdf_v1_points.to_file(\"datas_finales_points.geojson\", driver=\"GeoJSON\")\n",
    "# gdf_v1_polygons.to_file(\"datas_finales_polygons.geojson\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données en utilisant geopandas\n",
    "gdf_v3 = gpd.read_file(\"datas_finales.geojson\")\n",
    "\n",
    "# Séparer les géométries en points et polyggone\n",
    "points = gdf_v3[gdf_v3['geometry'].type == 'Point']\n",
    "polygons = gdf_v3[gdf_v3['geometry'].type == 'Polygon']\n",
    "\n",
    "# Enregistrer les sous-ensembles en tant que fichiers GeoJSON\n",
    "points.to_file(\"points.geojson\", driver=\"GeoJSON\")\n",
    "polygons.to_file(\"polygons.geojson\", driver=\"GeoJSON\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        RÉSULTATS\n",
    "\n",
    "Nous présentons ici les visualisations que nous avons pû obtenir en utilisant Tableau. Comme expliqué ensuite, il a fallu séparer les points et les polygons dans deux fichiers différents, nécessitant ainsi d'apporter deux sources de données dans Tableau (les fichiers comportant des éléments de géométrie de type différents ne sont pas encore supportés).\n",
    "\n",
    "Nous présentons ici 4 niveaux de zoom différents des cartes produites grâce à notre base de données. Le résultat est plutôt visuel.\n",
    "\n",
    "![Premier niveau de zoom](1.png)\n",
    "\n",
    "![Deuxième niveau de zoom](2.png)\n",
    "\n",
    "![Troisième niveau de zoom](3.png)\n",
    "\n",
    "![Quatrième niveau de zoom](4.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        PROBLÈMES RENCONTRÉS\n",
    "\n",
    "Durant le prétraitement ou dans la production de notre base, nous avons rencontré un problème d’optimisation. Le nombre d’éléments dans les bases est conséquent ce qui a rendu leurs analyses et traitements complexes. Les ordinateurs que nous avons à dispositions se rapprochent plus des notebook que d'ordinateurs destinés à la programmation. Cela à résulter en une certaine difficulté à traiter les bases d’un seul coup. En effet, pour ce qui est de l'association des données de valeurs foncières à un polygone lorsque le point de la première base est compris dans le polygone de la deuxième, cela nous a pris plus d’une nuit pour obtenir un résultat. Nous avons pour ce qui est des autres modifications des bases, d’abord travaillé avec des échantillons pour une question de rapidité d’exécution, et une fois un résultat satisfaisant obtenu, effectué le traitement sur la base de données. \n",
    "\n",
    "Nous avons aussi rencontré des problèmes pour utiliser le fichier généré dans Tableau. En effet, la solution de visualisation ne supporte pas encore les fichiers avec des objets géométriques de différents types. Nous avons alors dû séparer dans deux fichiers les valeurs associées à des points qui ne correspondaient pas à des bâtiments décrits dans la base de données des référentiels bâtiments de Rennes ainsi que les valeurs qui ont bien été associées à des polygones.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        EVOLUTIONS\n",
    "\n",
    "De manière à faire évoluer encore notre projet, la prochaine étape est le passage à l'échelle de plusieurs départements. Le fonctionnement serait le même pour le traitement que nous avons fait au niveau d'un département. La difficulté résiderait alors dans la recherche de bases de données de référentiels de bâtiments. Pour contourner cette erreur, nous avons sélectionné dans le traitement plus avancé la colonne 'id_parcelle' qui est une colonne qui a été ajoutée par Etalab aux données DVF originales dans le but de faire correspondre cet identifiant avec les plans cadastraux des parcelles aussi présents en open data sur les sites du gouvernement. La valeur foncière affichée sur l'outil de visualisation Tableau serait donc limitée à la parcelle et non plus au bâtiment lorsqu'il y aurait correspondance. On pourrait imaginer un traitement qui permettrait d'associer les valeurs aux bâtiments en priorité lorsqu'il y a des données disponibles et sinon aux parcelles des plans cadastraux.\n",
    "\n",
    "Un autre point clé pour passer à l'échelle le projet serait de rajouter des filtres selon le 'niveau de zoom'. C'est une fonctionnalité qui est accessible dans Tableau. Si l'on s'intéresse toujours à la représentation des valeurs foncières, il faudrait imaginer un traitement supplémentaire qui associerait la moyenne des valeurs foncières d'un département à une forme qui représente le département. De manière à pouvoir visualiser les différences de prix moyen entre départements à l'échelle de la France par exemple. La même logique pourrait être appliquée pour les régions voire même les pays. Il faudrait alors gérer en plus l'unité dans laquelle seraient exprimées les valeurs foncières. Dans l'outil de visualisation tableau, il serait alors possible de cliquer sur la France par exemple pour qu'un zoom sur les formes représentant les régions apparaisse. Puis en cliquant sur les régions un filtre sur les formes des départements apparaît. Et enfin un clique sur un département affichera le niveau de visualisation que nous avons mis en place.\n",
    "\n",
    "Un point d'amélioration sur lequel nous n'avons pas eu réellement le temps de travailler mais qui aurait pu être intéressant est la définition d'un niveau de granularité intermédiaire entre le département et le niveau que nous avons mis en place. Le but serait d'avoir des quartiers par exemple pour pouvoir visualiser les différences de valeurs entre les quartiers d'une ville."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f402da8765347f7f08cbce2a01da0c54883a4980987c7f2304281ccd14be9e18"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
